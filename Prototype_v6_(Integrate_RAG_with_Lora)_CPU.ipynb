{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DwEV3Iig4n-o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "!pip install -qU langchain tiktoken langchain_community langchain_chroma langchain-huggingface huggingface-hub sentence_transformers chromadb langchainhub transformers peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGx2rC4Z-bTz",
        "outputId": "05838656-6062-42f8-e550-911f97629a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel, PeftConfig\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mE5xhSl7bvqC"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the total number of tokens in the vector database\n",
        "def count_total_tokens_in_vectorstore(vectorstore, tokenizer):\n",
        "    # Retrieve all documents from the vector store\n",
        "    all_docs = vectorstore.get()['documents']\n",
        "\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Iterate over each document and calculate the number of tokens\n",
        "    for doc in all_docs:\n",
        "        tokens_in_doc = len(tokenizer.encode(doc))  # Tokenize the document content (which is a string)\n",
        "        total_tokens += tokens_in_doc\n",
        "\n",
        "    return total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "745247ef55d94fd09f7ec92120d05746",
            "7d906770b92040f68767241cebeff7d0",
            "40b956a60bff4880b72ffbbbc286ca4b",
            "595d5d00f4b24224a65cfbdcbb378913",
            "570edfacd05749019d7aa9f9e494ec06",
            "035fc17962f34ae1a1a76aeaeb328810",
            "d138a1fbf3004b4582b1a1725a6b951b",
            "65af85cbdfa1446fadf7106445df4c7a",
            "ba2c634ab42748f68d155150c17bb9c0",
            "ac308ac55c9b4d0da0d421ce3e6d8c82",
            "8c9246d56112485890002272220370c4"
          ]
        },
        "id": "sZq2XNAK_--a",
        "outputId": "7e88d511-92f6-4418-c746-e7f054ea0956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.db.impl.sqlite:⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "745247ef55d94fd09f7ec92120d05746"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "# Initialize embeddings\n",
        "embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "embedding_model_kwargs = {\"device\": \"cpu\"}\n",
        "embedding_encode_kwargs = {\"normalize_embeddings\": True}\n",
        "hf = HuggingFaceBgeEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=embedding_model_kwargs,\n",
        "    encode_kwargs=embedding_encode_kwargs\n",
        ")\n",
        "\n",
        "# Initialize vector store and retriever\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=\"/content/drive/MyDrive/UWA/Sem 4/Capstone/Project/vector1\",\n",
        "    embedding_function=hf\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "base_model = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# Load the fine-tuned Phi3 mini model with LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model, return_dict=True, device_map=device, torch_dtype=torch.float32)  # Changed device_map to CPU and dtype to float32\n",
        "lora_model = PeftModel.from_pretrained(model, \"ShilpaSandhya/phi3_5_mini_lora_chemical_eng\")\n",
        "\n",
        "pipeline = pipeline(\"text-generation\", model=lora_model, tokenizer=tokenizer, max_new_tokens=count_total_tokens_in_vectorstore(vectorstore, tokenizer)//10)  # Added device=0 to enforce CPU usage\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tnhbv16wAE_X"
      },
      "outputs": [],
      "source": [
        "# Define the RAG Chat Model class\n",
        "class RAGChatModel:\n",
        "    def __init__(self, retriever, llm, tokenizer, max_token_limit=count_total_tokens_in_vectorstore(vectorstore, tokenizer)//10):\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_limit = max_token_limit\n",
        "        self.current_token_count = 0\n",
        "        self.template_standard = \"\"\"\n",
        "        <|system|>\n",
        "        Answer the question and mustgive all the page numbers for the answer where this information is found based in the information provided in the context.\n",
        "        Providing all the page numbers is essential for the answer.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Providing all the page numbers is essential  for the answer.\n",
        "        <|end|>\n",
        "\n",
        "        <|user|>\n",
        "        Question: {question}\n",
        "        <|end|>\n",
        "\n",
        "        <|assistant|>\n",
        "        \"\"\"\n",
        "        self.template_exceeded = \"\"\"\n",
        "        <|system|>\n",
        "        Answer the question in detail; warn that information is not taken from the prescribed textbook and must provide the page numbers where they can find the correct information in the prescribed textbook.\n",
        "\n",
        "        Context: {context}\n",
        "        Providing all the page numbers is essential for the answer.\n",
        "        <|end|>\n",
        "\n",
        "        <|user|>\n",
        "        Question: {question}\n",
        "        <|end|>\n",
        "\n",
        "        <|assistant|>\n",
        "        \"\"\"\n",
        "\n",
        "    def num_tokens_from_string(self, string: str) -> int:\n",
        "        \"\"\"Returns the number of tokens in a text string using the tokenizer.\"\"\"\n",
        "        return len(self.tokenizer.encode(string))\n",
        "\n",
        "    def format_docs(self, docs, full_content=True):\n",
        "        \"\"\"Format the documents to be used as context in the prompt.\"\"\"\n",
        "        if full_content:\n",
        "            return \"\\n\\n\".join(f\"Information in Page number: {(doc.metadata['page']+1)}\\n{doc.page_content}\" for doc in docs)\n",
        "        else:\n",
        "            return \"Information available in prescribed textbook \" + \", \".join(f\"Page number: {doc.metadata['page']}\" for doc in docs)\n",
        "\n",
        "    def get_prompt(self, docs, question):\n",
        "        \"\"\"Generate the prompt based on token count and context formatting.\"\"\"\n",
        "        # Format the context with full content\n",
        "        context = self.format_docs(docs, full_content=True)\n",
        "        total_tokens_in_context = self.num_tokens_from_string(context)\n",
        "\n",
        "        # Add tokens to the running total\n",
        "        self.current_token_count += total_tokens_in_context\n",
        "\n",
        "        # Decide whether to use full content or only page numbers\n",
        "        if self.current_token_count > self.max_token_limit:\n",
        "            print(\"Token limit exceeded. Information from prescribed textbook will not be used.\")\n",
        "            # Reformat context to include only page numbers\n",
        "            context = self.format_docs(docs, full_content=False)\n",
        "            template = self.template_exceeded\n",
        "        else:\n",
        "            template = self.template_standard\n",
        "\n",
        "        # Create the prompt\n",
        "        prompt = template.format(context=context, question=question)\n",
        "        return prompt\n",
        "\n",
        "    def extract_clean_answer(self, raw_output):\n",
        "        \"\"\"Extract only the answer from the raw output.\"\"\"\n",
        "        assistant_tag = \"<|assistant|>\"\n",
        "        if assistant_tag in raw_output:\n",
        "            clean_answer = raw_output.split(assistant_tag)[-1].strip()\n",
        "            return clean_answer\n",
        "        return raw_output.strip()\n",
        "\n",
        "    def ask_question(self, question):\n",
        "        \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n",
        "        # Retrieve relevant documents\n",
        "        docs = self.retriever.invoke(question)\n",
        "\n",
        "        # Generate prompt based on token count\n",
        "        prompt = self.get_prompt(docs, question)\n",
        "\n",
        "        # Pass the prompt to the LLM\n",
        "        result = self.llm.generate([prompt])\n",
        "\n",
        "        # Extract the generated text\n",
        "        raw_answer = result.generations[0][0].text\n",
        "\n",
        "        # Get the clean answer\n",
        "        clean_answer = self.extract_clean_answer(raw_answer)\n",
        "\n",
        "        # Display the answer\n",
        "        display(Markdown(clean_answer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MAbb4UbqAVm6"
      },
      "outputs": [],
      "source": [
        "# Initialize the RAGChatModel\n",
        "rag_chat_model = RAGChatModel(retriever, llm, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJOMy2MJyls1",
        "outputId": "8b558dc1-fa1e-4889-9a46-9a177d92ba07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.9)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.39.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "KSMx6Ps3yT5k",
        "outputId": "4df81a51-33c0-4806-9f72-c11d514802a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b1df5ed315b331d5aa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b1df5ed315b331d5aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Function to shorten the question for the chat history display\n",
        "def get_short_overview(question, answer, max_length=50):\n",
        "    \"\"\"Generate a short summary of the question for the chat history.\"\"\"\n",
        "    # For simplicity, return the first few words of the question\n",
        "    short_question = (question[:max_length] + '...') if len(question) > max_length else question\n",
        "    return short_question\n",
        "\n",
        "# Function for the RAG model interaction\n",
        "def ask_question_gradio(history, question):\n",
        "    \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    docs = rag_chat_model.retriever.invoke(question)\n",
        "\n",
        "    # Generate prompt based on token count\n",
        "    prompt = rag_chat_model.get_prompt(docs, question)\n",
        "\n",
        "    # Pass the prompt to the LLM\n",
        "    result = rag_chat_model.llm.generate([prompt])\n",
        "\n",
        "    # Extract the generated text\n",
        "    raw_answer = result.generations[0][0].text\n",
        "\n",
        "    # Get the clean answer\n",
        "    clean_answer = rag_chat_model.extract_clean_answer(raw_answer)\n",
        "\n",
        "    # Add the question and answer to the conversation history\n",
        "    history.append((question, clean_answer))\n",
        "\n",
        "    # Generate a short summary for the chat history section (only from the question)\n",
        "    short_overview = get_short_overview(question, clean_answer)\n",
        "\n",
        "    # Append the short overview to the chat history display (only questions' summaries)\n",
        "    chat_history = \"\\n\\n\".join([get_short_overview(q, a) for q, a in history])\n",
        "\n",
        "    # Return the updated history, overview history for the left panel, and clear the user input\n",
        "    return history, chat_history, \"\"  # The empty string clears the input box\n",
        "\n",
        "# Create a Gradio Blocks interface for chat-like interaction\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <h1 style='text-align: center;'>Lora ChemNerd</h1>\n",
        "        <p style='text-align: center;'>Ask any question and get a response from the RAG model.</p>\n",
        "        \"\"\",\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # Sidebar for chat history\n",
        "        with gr.Column(scale=1, min_width=200):\n",
        "            gr.Markdown(\"### Chat History Overview\")\n",
        "            history_display = gr.Textbox(label=\"Chat History\", lines=20, interactive=False)  # Non-editable\n",
        "\n",
        "        # Main Chatbot Area\n",
        "        with gr.Column(scale=2):\n",
        "            # Chatbot layout with conversation history\n",
        "            chatbot = gr.Chatbot(label=\"Unsloth ChemNerd Chat\")\n",
        "\n",
        "            # Text input for user questions\n",
        "            with gr.Row():\n",
        "                user_input = gr.Textbox(\n",
        "                    placeholder=\"Ask your question...\",\n",
        "                    label=\"Type your message here:\"\n",
        "                )\n",
        "\n",
        "                # Button to submit the question\n",
        "                submit_button = gr.Button(\"Send\")\n",
        "\n",
        "            # Maintain the conversation history\n",
        "            history_state = gr.State([])\n",
        "\n",
        "            # Link the components: send input, update chatbot, clear textbox, and update history display\n",
        "            submit_button.click(\n",
        "                ask_question_gradio,\n",
        "                inputs=[history_state, user_input],\n",
        "                outputs=[chatbot, history_display, user_input],\n",
        "                scroll_to_output=True\n",
        "            )\n",
        "            user_input.submit(\n",
        "                ask_question_gradio,\n",
        "                inputs=[history_state, user_input],\n",
        "                outputs=[chatbot, history_display, user_input],\n",
        "                scroll_to_output=True\n",
        "            )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_rtxszlAY2E",
        "outputId": "5144b7e6-39ef-4ea8-a59a-dc3ac5ee5e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your question: exit\n"
          ]
        }
      ],
      "source": [
        "# Start the interactive chat\n",
        "print(\"Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\")\n",
        "while True:\n",
        "  print(\"\\n\\n\")\n",
        "  question = input(\"Your question: \")\n",
        "  if question.lower() == 'exit':\n",
        "    print(\"Exiting the chat.\")\n",
        "    break\n",
        "  rag_chat_model.ask_question(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-1jL_wcZbB7"
      },
      "source": [
        "How do we determine the breakthru point for an absorption bed?\n",
        "\n",
        "what is entropy from the perspective of a molecule?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "745247ef55d94fd09f7ec92120d05746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d906770b92040f68767241cebeff7d0",
              "IPY_MODEL_40b956a60bff4880b72ffbbbc286ca4b",
              "IPY_MODEL_595d5d00f4b24224a65cfbdcbb378913"
            ],
            "layout": "IPY_MODEL_570edfacd05749019d7aa9f9e494ec06"
          }
        },
        "7d906770b92040f68767241cebeff7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_035fc17962f34ae1a1a76aeaeb328810",
            "placeholder": "​",
            "style": "IPY_MODEL_d138a1fbf3004b4582b1a1725a6b951b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "40b956a60bff4880b72ffbbbc286ca4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65af85cbdfa1446fadf7106445df4c7a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba2c634ab42748f68d155150c17bb9c0",
            "value": 2
          }
        },
        "595d5d00f4b24224a65cfbdcbb378913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac308ac55c9b4d0da0d421ce3e6d8c82",
            "placeholder": "​",
            "style": "IPY_MODEL_8c9246d56112485890002272220370c4",
            "value": " 2/2 [00:41&lt;00:00, 18.18s/it]"
          }
        },
        "570edfacd05749019d7aa9f9e494ec06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035fc17962f34ae1a1a76aeaeb328810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d138a1fbf3004b4582b1a1725a6b951b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65af85cbdfa1446fadf7106445df4c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2c634ab42748f68d155150c17bb9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac308ac55c9b4d0da0d421ce3e6d8c82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9246d56112485890002272220370c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}